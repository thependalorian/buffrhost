#!/usr/bin/env python3
"""
Legacy Migration Cleanup Script for Buffr Host
This script verifies that consolidated migrations were successful and then cleans up legacy files.
"""

import os
import sys
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
import logging
import shutil
from pathlib import Path

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_database_url():
    """Get database URL from environment variables"""
    database_url = os.getenv('DATABASE_URL') or os.getenv('NEON_DATABASE_URL')
    if not database_url:
        logger.error("DATABASE_URL or NEON_DATABASE_URL environment variable not set")
        sys.exit(1)
    return database_url

def verify_migrations_success():
    """Verify that consolidated migrations were successful"""
    database_url = get_database_url()
    
    try:
        # Connect to database
        logger.info("Connecting to database to verify migrations...")
        conn = psycopg2.connect(database_url)
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cursor = conn.cursor()
        
        # Check if migration history table exists
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'migration_history'
            );
        """)
        migration_history_exists = cursor.fetchone()[0]
        
        if not migration_history_exists:
            logger.error("‚ùå Migration history table not found. Migrations may not have been run.")
            return False
        
        # Check applied migrations
        cursor.execute("SELECT migration_file, success FROM migration_history ORDER BY applied_at")
        migrations = cursor.fetchall()
        
        logger.info(f"Found {len(migrations)} recorded migrations:")
        successful_migrations = 0
        failed_migrations = 0
        
        for migration_file, success in migrations:
            status = "‚úÖ" if success else "‚ùå"
            logger.info(f"  {status} {migration_file}")
            if success:
                successful_migrations += 1
            else:
                failed_migrations += 1
        
        if failed_migrations > 0:
            logger.error(f"‚ùå {failed_migrations} migrations failed. Cannot proceed with cleanup.")
            return False
        
        # Verify core tables exist
        logger.info("Verifying core tables...")
        core_tables = [
            'tenants', 'users', 'buffr_ids', 'properties', 
            'property_images', 'property_features'
        ]
        
        missing_tables = []
        for table in core_tables:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table,))
            if not cursor.fetchone()[0]:
                missing_tables.append(table)
        
        if missing_tables:
            logger.error(f"‚ùå Missing core tables: {missing_tables}")
            return False
        
        # Verify user roles enum
        logger.info("Verifying user roles enum...")
        cursor.execute("SELECT unnest(enum_range(NULL::user_role_enum)) as role_values")
        roles = [row[0] for row in cursor.fetchall()]
        
        required_roles = ['super_admin', 'admin', 'manager', 'staff', 'property_owner', 'customer', 'guest']
        missing_roles = [role for role in required_roles if role not in roles]
        
        if missing_roles:
            logger.error(f"‚ùå Missing user roles: {missing_roles}")
            return False
        
        logger.info(f"‚úÖ All required user roles present: {roles}")
        
        # Verify analytics views
        logger.info("Verifying analytics views...")
        analytics_views = ['user_roles_summary', 'tenant_user_summary', 'property_performance']
        
        missing_views = []
        for view in analytics_views:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.views 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (view,))
            if not cursor.fetchone()[0]:
                missing_views.append(view)
        
        if missing_views:
            logger.warning(f"‚ö†Ô∏è  Missing analytics views: {missing_views}")
        else:
            logger.info("‚úÖ All analytics views present")
        
        # Check table counts
        cursor.execute("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
        table_count = cursor.fetchone()[0]
        logger.info(f"‚úÖ Total tables in database: {table_count}")
        
        # Check if we have sample data
        cursor.execute("SELECT COUNT(*) FROM properties")
        property_count = cursor.fetchone()[0]
        logger.info(f"‚úÖ Properties in database: {property_count}")
        
        cursor.execute("SELECT COUNT(*) FROM users")
        user_count = cursor.fetchone()[0]
        logger.info(f"‚úÖ Users in database: {user_count}")
        
        logger.info("üéâ All migrations verified successfully!")
        return True
        
    except psycopg2.Error as e:
        logger.error(f"‚ùå Database error during verification: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Unexpected error during verification: {e}")
        return False
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

def backup_legacy_files():
    """Create a backup of legacy files before cleanup"""
    logger.info("Creating backup of legacy migration files...")
    
    backup_dir = Path("migrations/legacy_backup")
    backup_dir.mkdir(exist_ok=True)
    
    legacy_dir = Path("migrations/legacy")
    if legacy_dir.exists():
        shutil.copytree(legacy_dir, backup_dir / "legacy", dirs_exist_ok=True)
        logger.info(f"‚úÖ Legacy files backed up to {backup_dir}")
        return True
    else:
        logger.warning("‚ö†Ô∏è  No legacy files found to backup")
        return False

def cleanup_legacy_files():
    """Clean up legacy migration files and directories"""
    logger.info("Starting legacy file cleanup...")
    
    cleanup_items = [
        # Legacy migration directories
        "migrations/legacy",
        "backend/migrations",
        "frontend/sql",
        
        # Individual migration files that are now consolidated
        "CRITICAL_FIXES_MIGRATION.sql",
        "apply_critical_fixes.py",
        
        # Old migration runners
        "run_neon_migrations.py",
        "run_neon_migrations_fixed.py", 
        "run_neon_migrations_simple.py",
    ]
    
    cleaned_items = []
    failed_items = []
    
    for item in cleanup_items:
        item_path = Path(item)
        if item_path.exists():
            try:
                if item_path.is_dir():
                    shutil.rmtree(item_path)
                    logger.info(f"‚úÖ Removed directory: {item}")
                else:
                    item_path.unlink()
                    logger.info(f"‚úÖ Removed file: {item}")
                cleaned_items.append(item)
            except Exception as e:
                logger.error(f"‚ùå Failed to remove {item}: {e}")
                failed_items.append(item)
        else:
            logger.info(f"‚è≠Ô∏è  Skipped (not found): {item}")
    
    return cleaned_items, failed_items

def update_documentation():
    """Update documentation to reflect the cleanup"""
    logger.info("Updating documentation...")
    
    # Update README to remove references to old migration files
    readme_path = Path("README.md")
    if readme_path.exists():
        with open(readme_path, 'r') as f:
            content = f.read()
        
        # Remove references to old migration paths
        content = content.replace("backend/migrations/", "migrations/consolidated/")
        content = content.replace("frontend/sql/", "migrations/consolidated/")
        
        with open(readme_path, 'w') as f:
            f.write(content)
        
        logger.info("‚úÖ Updated README.md")
    
    # Create cleanup summary
    cleanup_summary = """
# Legacy Migration Cleanup Summary

## ‚úÖ Cleanup Completed Successfully

### Files and Directories Removed:
- `migrations/legacy/` - Legacy migration files (backed up to `migrations/legacy_backup/`)
- `backend/migrations/` - Original backend migration files
- `frontend/sql/` - Original frontend SQL files
- `CRITICAL_FIXES_MIGRATION.sql` - Consolidated into new system
- `apply_critical_fixes.py` - Replaced by consolidated migration runner
- Old migration runner scripts

### Current Migration System:
- **Location**: `migrations/consolidated/`
- **Runner**: `migrations/run_consolidated_migrations.py`
- **Documentation**: `migrations/README.md`

### Verification Results:
- ‚úÖ All consolidated migrations applied successfully
- ‚úÖ Core tables created and functional
- ‚úÖ User roles enum complete with all required roles
- ‚úÖ Analytics views operational
- ‚úÖ Database schema complete and production-ready

### Next Steps:
1. Use `migrations/run_consolidated_migrations.py` for all future migrations
2. Reference `migrations/README.md` for documentation
3. Legacy files are backed up in `migrations/legacy_backup/` if needed

---
**Cleanup Date**: {date}
**Status**: ‚úÖ COMPLETED SUCCESSFULLY
""".format(date=__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    
    with open("LEGACY_CLEANUP_SUMMARY.md", 'w') as f:
        f.write(cleanup_summary)
    
    logger.info("‚úÖ Created LEGACY_CLEANUP_SUMMARY.md")

def main():
    """Main cleanup function"""
    logger.info("üßπ Starting Buffr Host Legacy Migration Cleanup")
    logger.info("=" * 60)
    
    # Step 1: Verify migrations were successful
    logger.info("Step 1: Verifying consolidated migrations...")
    if not verify_migrations_success():
        logger.error("‚ùå Migration verification failed. Cannot proceed with cleanup.")
        logger.error("Please fix migration issues before running cleanup.")
        sys.exit(1)
    
    # Step 2: Create backup of legacy files
    logger.info("Step 2: Creating backup of legacy files...")
    backup_legacy_files()
    
    # Step 3: Clean up legacy files
    logger.info("Step 3: Cleaning up legacy files...")
    cleaned_items, failed_items = cleanup_legacy_files()
    
    # Step 4: Update documentation
    logger.info("Step 4: Updating documentation...")
    update_documentation()
    
    # Summary
    logger.info("=" * 60)
    logger.info("üéâ LEGACY CLEANUP COMPLETED SUCCESSFULLY!")
    logger.info("=" * 60)
    logger.info(f"‚úÖ Cleaned up {len(cleaned_items)} items")
    if failed_items:
        logger.warning(f"‚ö†Ô∏è  Failed to clean up {len(failed_items)} items: {failed_items}")
    
    logger.info("")
    logger.info("üìÅ Current migration system:")
    logger.info("   - Location: migrations/consolidated/")
    logger.info("   - Runner: migrations/run_consolidated_migrations.py")
    logger.info("   - Documentation: migrations/README.md")
    logger.info("")
    logger.info("üîÑ For future migrations:")
    logger.info("   python migrations/run_consolidated_migrations.py")
    logger.info("")
    logger.info("üìã Legacy files backed up to: migrations/legacy_backup/")

if __name__ == "__main__":
    main()
